{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Textgenerator/harrypotter.txt'\n",
    "with open(path,'r',encoding='gb18030') as f:\n",
    "    article = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sentence(text):\n",
    "#     sentences = []\n",
    "    raw_sentence = re.findall(\"[\\w+[\\，\\、\\\"]*\\w+[\\！\\。\\？]\\\"*]*\",text)\n",
    "#     for sentence in raw_sentence:\n",
    "#         sentences.append(''.join(re.findall(\"\\w+\",sentence)))\n",
    "    return raw_sentence\n",
    "\n",
    "with open(\"./text_clean.txt\",'w') as f:\n",
    "    clean = text2sentence(article)\n",
    "    f.write(str(clean))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InAndOut(text):\n",
    "    into_sentences = text2sentence(text)\n",
    "    Input = []\n",
    "    Output = []\n",
    "    for line in into_sentences:\n",
    "        tmp = list(jieba.cut(line))\n",
    "        tmp2 = tmp[1:]\n",
    "        tmp2.append('<END>')\n",
    "        Input.append(tmp)\n",
    "        Output.append(tmp2)\n",
    "    return Input,Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.729 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "Input, Output = InAndOut(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(Input+Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(jieba.cut(article))\n",
    "data = [i for i in data if i != ' ' and i != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    def __init__(self,sents_in,sents_out):\n",
    "        self.sents_in = sents_in\n",
    "        self.sents_out = sents_out\n",
    "        self.vocab = Dictionary(sents_in+sents_out)\n",
    "        self.max_len = max(len(sent) for sent in sents_in)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        vectorize_x = self.one_hot(self.vocab,self.sents_in[index])\n",
    "        x = self.pad_sequence(vectorize_x,self.max_len)\n",
    "        vectorize_y = self.one_hot(self.vocab,self.sents_out[index])\n",
    "        y = self.pad_sequence(vectorize_y,self.max_len)\n",
    "        return x,y\n",
    "    \n",
    "    def pad_sequence(self, vectorized_sent, max_len):\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, max_len - len(vectorized_sent))\n",
    "        return F.pad(vectorized_sent, pad_dim, 'constant',value=37178)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sents_in)\n",
    "        \n",
    "    def one_hot(self, vocab, tokens):\n",
    "        return torch.tensor(vocab.doc2idx(tokens, unknown_word_index=1))\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_potter = Mydataset(Input,Output)\n",
    "dataloader = DataLoader(harry_potter,10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size,num_layers=1):\n",
    "        super(RNNLM,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.RNN = nn.LSTM(embedding_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Dropout(),nn.Linear(hidden_size, vocab_size),nn.LogSoftmax())\n",
    "        \n",
    "    def forward(self,x,h=None):\n",
    "        x = self.embedding(x)\n",
    "        self.batch_size = x.shape[0]\n",
    "        o,h = self.RNN(x,h)\n",
    "        o = o.contiguous().view(o.shape[0]*o.shape[1],o.shape[2])\n",
    "        out = self.fc(o)\n",
    "        return out, h \n",
    "Model = RNNLM(len(harry_potter.vocab),300,100).cuda()\n",
    "Cost = nn.NLLLoss()\n",
    "Optimizer = torch.optim.Adam(Model.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss=  tensor(0.5490)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=  tensor(0.3797)\n",
      "loss=  tensor(0.3465)\n",
      "loss=  tensor(0.3443)\n",
      "loss=  tensor(0.3456)\n",
      "loss=  tensor(0.3463)\n",
      "loss=  tensor(0.3462)\n",
      "loss=  tensor(0.3445)\n",
      "loss=  tensor(0.3492)\n",
      "loss=  tensor(0.3492)\n",
      "loss=  tensor(0.3506)\n",
      "loss=  tensor(0.3511)\n",
      "loss=  tensor(0.3481)\n",
      "loss=  tensor(0.3520)\n",
      "loss=  tensor(0.3530)\n",
      "epoch:  1\n",
      "loss=  tensor(0.5405)\n",
      "loss=  tensor(0.3337)\n",
      "loss=  tensor(0.3336)\n",
      "loss=  tensor(0.3350)\n",
      "loss=  tensor(0.3353)\n",
      "loss=  tensor(0.3325)\n",
      "loss=  tensor(0.3362)\n",
      "loss=  tensor(0.3370)\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    print(\"epoch: \",epoch)\n",
    "    for i,(x,y) in enumerate(dataloader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda().view(-1)\n",
    "        output, hidden = Model(x)\n",
    "        loss = Cost(output,y)\n",
    "        running_loss += loss.detach().cpu()\n",
    "        loss.backward(retain_graph=True)\n",
    "        Optimizer.step()\n",
    "        Optimizer.zero_grad()\n",
    "        if i%500 == 0:\n",
    "            print(\"loss= \",running_loss/(i+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "#     dataloader = DataLoader(harry_potter,batch_size=1,shuffle=False)\n",
    "#     for x,y in dataloader:\n",
    "#         x = x.cuda()\n",
    "#         print(x)\n",
    "#         break\n",
    "    x = torch.tensor([[1]]).cuda()\n",
    "    i = 0\n",
    "    index = x.cpu().tolist()[0]\n",
    "    text = index\n",
    "    \n",
    "    while i < 100:\n",
    "        i+=1\n",
    "        out,hidden = Model(x)\n",
    "        maxim,index = torch.max(out[-1],0)\n",
    "        index = index.cpu().tolist()\n",
    "        x = torch.cat([x[0],torch.tensor([index]).cuda()])\n",
    "        x = x.view(1,-1)\n",
    "        text.append(index)\n",
    "    print(''.join(harry_potter.unvectorize(text)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人的人都在他的面前，他看到了他的他，他的魔杖，他就在他的魔杖，他的眼睛都在他的魔杖里，他看到了，他的眼睛是，他的眼睛是的的。<END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END><END>\n"
     ]
    }
   ],
   "source": [
    "predict()\n",
    "# x = torch.tensor([[    2,     7,     6,    12]]).cuda()\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  2,  4, 23, 34,  1])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [10,2,4,23,34]\n",
    "x = torch.tensor(x)\n",
    "x = torch.cat([x,torch.tensor([1])])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
